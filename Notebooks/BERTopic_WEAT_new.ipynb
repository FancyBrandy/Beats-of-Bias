{"cells":[{"cell_type":"markdown","metadata":{"id":"Y3VGFZ1USMTu"},"source":["# **Prepare data**\n"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# Following snippet to be used with gdrive:\n","# gdrive_path='/content/gdrive/MyDrive/Bertopic/shared_work/'\n","\n","# # This will mount your google drive under 'MyDrive'\n","# drive.mount('/content/gdrive', force_remount=True)\n","# # In order to access the files in this notebook we have to navigate to the correct folder\n","# os.chdir(gdrive_path)\n","# # Check manually if all files are present\n","# print(sorted(os.listdir()))\n","\n","\n","# Use this when there's no gdrive:\n","\n","dataset_path = '/content/drive/MyDrive/Praktikum - NLP Applications/Datasets/genius_and_wasabi/concatenated_chunks.csv'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2eZbQUOv2WU","executionInfo":{"status":"ok","timestamp":1701383937621,"user_tz":-60,"elapsed":19335,"user":{"displayName":"Adithi Satish","userId":"11963860189572564168"}},"outputId":"c334b3a8-95f1-45ca-c309-d8c7ff569623"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Common Drive instructions:\n","\n","!pip install pandas numpy bertopic\n","!pip install gensim nltk matplotlib seaborn"],"metadata":{"id":"PzbHaPkp8NLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pandas bertopic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjvTAsN1xeE7","executionInfo":{"status":"ok","timestamp":1701384064067,"user_tz":-60,"elapsed":122750,"user":{"displayName":"Adithi Satish","userId":"11963860189572564168"}},"outputId":"89085c8f-7a12-4cfa-e352-eca9c7e6cd94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Collecting bertopic\n","  Downloading bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m704.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Collecting hdbscan>=0.8.29 (from bertopic)\n","  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting umap-learn>=0.5.0 (from bertopic)\n","  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n","Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.1)\n","Collecting sentence-transformers>=0.4.1 (from bertopic)\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n","Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n","  Using cached Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.3)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (23.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.35.2)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.16.0+cu118)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n","Collecting sentencepiece (from sentence-transformers>=0.4.1->bertopic)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.19.4)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n","Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n","  Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.7)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n","Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn\n","  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039170 sha256=06ba92bde3bd4746a30f22cd22912f47b75414e36b6d6ee3f28a7a5cb8f8091b\n","  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=f55a5ddce8cea495ffb0702fe72be8fd155bacd45126215b40005f9edfc78042\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86831 sha256=7b169e9a58d4f8d773c5c9e722c58033651acc4dfc15f9d3b22ed4fa3069bcb4\n","  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n","Successfully built hdbscan sentence-transformers umap-learn\n","Installing collected packages: sentencepiece, cython, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n","  Attempting uninstall: cython\n","    Found existing installation: Cython 3.0.5\n","    Uninstalling Cython-3.0.5:\n","      Successfully uninstalled Cython-3.0.5\n","Successfully installed bertopic-0.16.0 cython-0.29.36 hdbscan-0.8.33 pynndescent-0.5.11 sentence-transformers-2.2.2 sentencepiece-0.1.99 umap-learn-0.5.5\n"]}]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjxLuNppbUb_","executionInfo":{"status":"ok","timestamp":1701384076957,"user_tz":-60,"elapsed":12899,"user":{"displayName":"Adithi Satish","userId":"11963860189572564168"}},"outputId":"edb6d685-0e00-4cdb-9b3e-50624a6f6ed7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y2miiytzbW9m","executionInfo":{"status":"ok","timestamp":1701384079824,"user_tz":-60,"elapsed":2871,"user":{"displayName":"Adithi Satish","userId":"11963860189572564168"}},"outputId":"f642c4d3-0977-45f9-b06d-173c26713a0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["!pip install --upgrade tensorflow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hn5o_ANeTuzo","executionInfo":{"status":"ok","timestamp":1701384141115,"user_tz":-60,"elapsed":61293,"user":{"displayName":"Adithi Satish","userId":"11963860189572564168"}},"outputId":"65429992-fc6c-4ca4-adaa-66cdcd0ed499"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Collecting tensorflow\n","  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n","Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n","  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n","  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n","Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n","  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 0.4.6\n","    Uninstalling google-auth-oauthlib-0.4.6:\n","      Successfully uninstalled google-auth-oauthlib-0.4.6\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.0\n","    Uninstalling tensorboard-2.12.0:\n","      Successfully uninstalled tensorboard-2.12.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","Successfully installed google-auth-oauthlib-1.1.0 keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n"]}]},{"cell_type":"code","source":["!pip install umap-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkTQsukWY3CJ","executionInfo":{"status":"ok","timestamp":1701384224198,"user_tz":-60,"elapsed":14590,"user":{"displayName":"Adithi Satish","userId":"11963860189572564168"}},"outputId":"4f3002c3-fd0d-4930-dce2-4ca77e6755cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.3)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from bertopic import BERTopic\n","from umap import UMAP\n","\n","# set path of the file\n","csv_file_path = 'concatenated_chunks.csv'\n","# df = pd.read_csv(csv_file_path) # -> Uncomment for gdrive\n","df = pd.read_csv(dataset_path)\n","\n","documents = df['lyrics'].tolist()  # Convert the text column to a list"],"metadata":{"id":"gwVLYZHExIpM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Data Preprocessing**\n","This involves removing the explicit song structure from the lyrics column and initialising the CountVectorizer so that stop word removal is handled internally by BERTopic"],"metadata":{"id":"K4JSLS4VNkmW"}},{"cell_type":"code","source":["# Removing explicit song structure as it is not important information\n","import re\n","\n","def remove_explicit_song_structure(lyrics):\n","  pattern = r'\\[.+?\\]'\n","  cleaned_lyrics = re.sub(pattern, '', lyrics)\n","  return cleaned_lyrics\n","\n","df['cleaned_lyrics'] = df['lyrics'].astype(str).apply(remove_explicit_song_structure)\n","df.head()"],"metadata":{"id":"-l1RmrFNc6Tz","executionInfo":{"status":"ok","timestamp":1701384227530,"user_tz":-60,"elapsed":3351,"user":{"displayName":"Adithi Satish","userId":"11963860189572564168"}},"colab":{"base_uri":"https://localhost:8080/","height":310},"outputId":"bd45b106-9582-4954-bfc3-a2e063c029ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0.1  Unnamed: 0              title genre   artist  year   views  \\\n","0             0           0          Killa Cam   rap  Cam'ron  2004  173166   \n","1             1           1       Down and Out   rap  Cam'ron  2004  144404   \n","2             2           2        Family Ties   rap  Cam'ron  2004   41960   \n","3             3           3  Rockin and Rollin   rap  Cam'ron  1998    6399   \n","4             4           4      Lord You Know   rap  Cam'ron  2004   11882   \n","\n","                                       features  \\\n","0                   {\"Cam\\\\'ron\",\"Opera Steve\"}   \n","1  {\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}   \n","2                     {\"Cam\\\\'ron\",\"Lady Wray\"}   \n","3                                 {\"Cam\\\\'ron\"}   \n","4          {\"Cam\\\\'ron\",\"Juelz Santana\",Jaheim}   \n","\n","                                              lyrics language gender  \\\n","0  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...       en   Male   \n","1  [Produced by Kanye West and Brian Miller]\\n\\n[...       en   Male   \n","2  [Verse 1: Cam'ron]\\nKilla, Dipset\\nMan I spit ...       en   Male   \n","3  [Verse 1]\\nAy yo you wonder who I are\\nI guzzl...       en   Male   \n","4  [Chorus: Jaheim]\\nNow Lord you know, just how ...       en   Male   \n","\n","                                      cleaned_lyrics  \n","0  \\nKilla Cam, Killa Cam, Cam\\nKilla Cam, Killa ...  \n","1  \\n\\n\\nUgh, Killa!\\nBaby!\\nKanye, this that 197...  \n","2  \\nKilla, Dipset\\nMan I spit that pimp talk, yo...  \n","3  \\nAy yo you wonder who I are\\nI guzzle up at t...  \n","4  \\nNow Lord you know, just how hard I try\\nTo l...  "],"text/html":["\n","  <div id=\"df-5b1de5e4-ba4e-40db-87a4-c435e626414b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Unnamed: 0</th>\n","      <th>title</th>\n","      <th>genre</th>\n","      <th>artist</th>\n","      <th>year</th>\n","      <th>views</th>\n","      <th>features</th>\n","      <th>lyrics</th>\n","      <th>language</th>\n","      <th>gender</th>\n","      <th>cleaned_lyrics</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Killa Cam</td>\n","      <td>rap</td>\n","      <td>Cam'ron</td>\n","      <td>2004</td>\n","      <td>173166</td>\n","      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n","      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n","      <td>en</td>\n","      <td>Male</td>\n","      <td>\\nKilla Cam, Killa Cam, Cam\\nKilla Cam, Killa ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Down and Out</td>\n","      <td>rap</td>\n","      <td>Cam'ron</td>\n","      <td>2004</td>\n","      <td>144404</td>\n","      <td>{\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}</td>\n","      <td>[Produced by Kanye West and Brian Miller]\\n\\n[...</td>\n","      <td>en</td>\n","      <td>Male</td>\n","      <td>\\n\\n\\nUgh, Killa!\\nBaby!\\nKanye, this that 197...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Family Ties</td>\n","      <td>rap</td>\n","      <td>Cam'ron</td>\n","      <td>2004</td>\n","      <td>41960</td>\n","      <td>{\"Cam\\\\'ron\",\"Lady Wray\"}</td>\n","      <td>[Verse 1: Cam'ron]\\nKilla, Dipset\\nMan I spit ...</td>\n","      <td>en</td>\n","      <td>Male</td>\n","      <td>\\nKilla, Dipset\\nMan I spit that pimp talk, yo...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>Rockin and Rollin</td>\n","      <td>rap</td>\n","      <td>Cam'ron</td>\n","      <td>1998</td>\n","      <td>6399</td>\n","      <td>{\"Cam\\\\'ron\"}</td>\n","      <td>[Verse 1]\\nAy yo you wonder who I are\\nI guzzl...</td>\n","      <td>en</td>\n","      <td>Male</td>\n","      <td>\\nAy yo you wonder who I are\\nI guzzle up at t...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>Lord You Know</td>\n","      <td>rap</td>\n","      <td>Cam'ron</td>\n","      <td>2004</td>\n","      <td>11882</td>\n","      <td>{\"Cam\\\\'ron\",\"Juelz Santana\",Jaheim}</td>\n","      <td>[Chorus: Jaheim]\\nNow Lord you know, just how ...</td>\n","      <td>en</td>\n","      <td>Male</td>\n","      <td>\\nNow Lord you know, just how hard I try\\nTo l...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b1de5e4-ba4e-40db-87a4-c435e626414b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5b1de5e4-ba4e-40db-87a4-c435e626414b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5b1de5e4-ba4e-40db-87a4-c435e626414b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-92fd358e-def0-48f1-9f49-2dc7a1e711c2\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-92fd358e-def0-48f1-9f49-2dc7a1e711c2')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-92fd358e-def0-48f1-9f49-2dc7a1e711c2 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Initializing the CountVectorizer with English stop words to pass as a parameter to BERTopic\n","count_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n","# Initialising the UMAP constructor to set the random seed - this is so the results are reproducible\n","\n","umap = UMAP(n_neighbors=15,\n","            n_components=5,\n","            min_dist=0.0,\n","            metric='cosine',\n","            low_memory=False,\n","            random_state=42)"],"metadata":{"id":"03__9YXSORRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SBcNmZJzSTY8"},"source":["# **Create Topics**\n","We select the \"english\" as the main language for our documents. If you want a multilingual model that supports 50+ languages, please select \"multilingual\" instead."]},{"cell_type":"code","source":["# Check if there are missing lyrics\n","df['cleaned_lyrics'].isna().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DSBDeWAOLbX","executionInfo":{"status":"ok","timestamp":1701384227531,"user_tz":-60,"elapsed":18,"user":{"displayName":"Adithi Satish","userId":"11963860189572564168"}},"outputId":"c465f611-5c0a-41c0-a93a-eb0371adb485"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Fitting and saving the BERTopic model\n","\n","model = BERTopic(vectorizer_model=count_vectorizer, language=\"english\", umap_model=umap)\n","model_path = '/content/drive/MyDrive/Praktikum - NLP Applications/Models/bertopic_initial'\n","documents = df['cleaned_lyrics'].astype(str).tolist()\n","\n","# Fitting BERTopic\n","topic_model = model.fit(documents)a\n","# Saving it using safetensors\n","embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n","topic_model.save(model_path, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)"],"metadata":{"id":"KMGPoHqnXEyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfhfzqkoSJ1I","colab":{"base_uri":"https://localhost:8080/","height":245},"outputId":"f48f2963-2a20-42ca-89dd-d3c59644cd08","executionInfo":{"status":"error","timestamp":1701389731202,"user_tz":-60,"elapsed":21,"user":{"displayName":"Danqing Chen","userId":"09845787644688696199"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3e4789196c00>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_lyrics'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lyrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flag the rows that have lyrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit BERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_lyrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_lyrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# you can obtain the embedding used for bertopic after fitting the model, NOTE: you can either do this or use embedding model directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BERTopic' is not defined"]}],"source":["model = BERTopic(vectorizer_model=count_vectorizer, language=\"english\", umap_model=umap)\n","# df['has_lyrics'] = ~df['lyrics'].isna() # flag the rows that have lyrics - Not needed as all rows have lyrics\n","# Fit BERTopic\n","documents = df['cleaned_lyrics'].astype(str).tolist()\n","topics, _ = model.fit_transform(documents) # you can obtain the embedding used for bertopic after fitting the model, NOTE: you can either do this or use embedding model directly\n","# df.loc[~df['has_lyrics'], 'topic'] = \"No Lyrics\"  # e.g., -1 or \"No Lyrics\"\n","# Assign topics only to rows where 'has_lyrics' is True\n","df.loc[df['cleaned_lyrics'], 'topic'] = topics"]},{"cell_type":"code","source":["# Save the topics using pickle\n","import pickle\n","with open('topics.pkl', 'wb') as topics_file:\n","    pickle.dump(topics, topics_file)\n","\n","# Now you can load the topics from the file in future runs\n","# with open('topics.pkl', 'rb') as topics_file:\n","#     topics = pickle.load(topics_file)\n","# df.loc[df['cleaned_lyrics'], 'topic'] = loaded_topics"],"metadata":{"id":"UzqOfISrQZRp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***To measure gender bias per topic in a dataset using BERTopic and WEAT (Word Embedding Association Test), you need to follow a series of steps. These include topic modeling with BERTopic, creating target and attribute word sets for WEAT, and then performing the WEAT analysis for each topic. To integrate WEAT analysis with BERTopic, you need a separate word embeddings model.***"],"metadata":{"id":"TzX0Be5dqKkg"}},{"cell_type":"markdown","source":["Step 1 Analyzing topic distribution across genders"],"metadata":{"id":"Y5GBbpWjeg98"}},{"cell_type":"code","source":["# Group by topics and gender, and count occurrences, aggregate lyrices by topic and gender\n","topic_gender_distribution = df.groupby(['topic', 'gender']).size().unstack(fill_value=0)\n","\n","#  normalize the counts to compare proportions rather than raw counts\n","topic_gender_distribution_normalized = topic_gender_distribution.div(topic_gender_distribution.sum(axis=1), axis=0)"],"metadata":{"id":"5jAm-eZ8egYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aggregate lyrics by topic and gender, groups the DataFrame by topic and gender and then concatenates all lyrics within each group.\n","aggregated_lyrics = df.groupby(['topic', 'gender'])['lyrics'].apply(lambda x: ' '.join(x)).reset_index()\n","# Extract embeddings for each group\n","embeddings = model._extract_embeddings(documents) # not recommended to use this method since it is an internal method and its use is not for standard operations"],"metadata":{"id":"ES4Kqmabj3x0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['embedding'] = list(embeddings)"],"metadata":{"id":"CnBJsoRxIqeV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Extract and Analyze Top Words per Topic-Gender Group\n","For each topic and gender group, extract the most representative words. These words will be used to measure bias."],"metadata":{"id":"Zw7BF12Plhzj"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","def get_top_words(text, n=20):\n","    vec = CountVectorizer(stop_words='english').fit([text])\n","    bag_of_words = vec.transform([text])\n","    sum_words = bag_of_words.sum(axis=0)\n","    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n","    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n","    return words_freq[:n]\n","aggregated_lyrics['top_words'] = aggregated_lyrics['lyrics'].apply(lambda x: get_top_words(x))"],"metadata":{"id":"O1DmLaVwlgi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install numpy gensim"],"metadata":{"id":"Dq9YelsLmUem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install matplotlib seaborn"],"metadata":{"id":"FJrbCQMWth0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cosine Similarity: This function computes the cosine similarity between two word embeddings.\n","\n","Mean Embedding Similarity: This function computes the average cosine similarity between each word in a target set and an attribute set.\n","\n","Differential Association: This calculates the WEAT score, which is the differential association between two sets of target words and two sets of attribute words.\n","\n","WEAT Effect Size: This calculates the effect size, a measure of how large the difference in associations is.\n","\n","Target and Attribute Sets: You need to define these sets based on your specific analysis goal."],"metadata":{"id":"s5nnpwzooYH8"}},{"cell_type":"code","source":["# manually define weat since the lib did not work\n","import numpy as np\n","from gensim.models import KeyedVectors\n","def cosine_similarity(embedding1, embedding2):\n","    # print(embedding1)\n","    # print(embedding2) # problem\n","    norm1 = np.linalg.norm(embedding1)\n","    norm2 = np.linalg.norm(embedding2)\n","    if norm1 == 0 or norm2 == 0:\n","        return 0  # Return 0 if either vector is a zero-vector\n","    return np.dot(embedding1, embedding2) / (norm1 * norm2)\n","\n","def mean_embedding_similarity(target_set, attribute_set, embeddings_model):\n","    total_similarity = 0\n","    count = 0\n","\n","    for target_word in target_set:\n","        if target_word in embeddings_model.key_to_index:\n","            target_embedding = embeddings_model[target_word]\n","            for attribute_word in attribute_set:\n","                if attribute_word in embeddings_model.key_to_index:\n","                    attribute_embedding = embeddings_model[attribute_word]\n","                    total_similarity += cosine_similarity(target_embedding, attribute_embedding)\n","                    count += 1\n","\n","    return total_similarity / count if count > 0 else 0\n","\n","\n","def differential_association(target_set_1, target_set_2, attribute_set_1, attribute_set_2, embeddings_model):\n","    return (mean_embedding_similarity(target_set_1, attribute_set_1, embeddings_model) -\n","            mean_embedding_similarity(target_set_1, attribute_set_2, embeddings_model)) - (\n","            mean_embedding_similarity(target_set_2, attribute_set_1, embeddings_model) -\n","            mean_embedding_similarity(target_set_2, attribute_set_2, embeddings_model))\n","\n","\n","def average_embedding(attribute_set, embeddings_model):\n","    embeddings = [embeddings_model[word] for word in attribute_set if word in embeddings_model.key_to_index]\n","    if embeddings:\n","        return np.mean(embeddings, axis=0)\n","    else:\n","        return np.zeros(embeddings_model.vector_size)  # Return zero vector if no embeddings\n","\n","def weat_effect_size(target_set_1, target_set_2, attribute_set_1, attribute_set_2, embeddings_model):\n","    attribute_set_1_avg_embedding = average_embedding(attribute_set_1, embeddings_model)\n","    attribute_set_2_avg_embedding = average_embedding(attribute_set_2, embeddings_model)\n","\n","    target_set_1_embeddings = [embeddings_model[word] for word in target_set_1 if word in embeddings_model.key_to_index]\n","    target_set_2_embeddings = [embeddings_model[word] for word in target_set_2 if word in embeddings_model.key_to_index]\n","\n","    # Calculate differences for target_set_1, in our case male words\n","    for word_embedding in target_set_1_embeddings:\n","        diff = cosine_similarity(word_embedding, attribute_set_1_avg_embedding) - cosine_similarity(word_embedding, attribute_set_2_avg_embedding)\n","        print(f\"Word embedding diff for target_set_1: {diff}\")\n","\n","    # Calculate differences for target_set_2, in our case female words\n","    for word_embedding in target_set_2_embeddings:\n","        diff = cosine_similarity(word_embedding, attribute_set_1_avg_embedding) - cosine_similarity(word_embedding, attribute_set_2_avg_embedding)\n","        print(f\"Word embedding diff for target_set_2: {diff}\")\n","\n","    mean_diff_1 = np.mean([cosine_similarity(word_embedding, attribute_set_1_avg_embedding) - cosine_similarity(word_embedding, attribute_set_2_avg_embedding)\n","                           for word_embedding in target_set_1_embeddings])\n","    mean_diff_2 = np.mean([cosine_similarity(word_embedding, attribute_set_1_avg_embedding) - cosine_similarity(word_embedding, attribute_set_2_avg_embedding)\n","                           for word_embedding in target_set_2_embeddings])\n","\n","    all_embeddings = np.concatenate([target_set_1_embeddings, target_set_2_embeddings])\n","    std_dev = np.std([cosine_similarity(word_embedding, attribute_set_1_avg_embedding) - cosine_similarity(word_embedding, attribute_set_2_avg_embedding)\n","                      for word_embedding in all_embeddings])\n","\n","    return (mean_diff_1 - mean_diff_2) / std_dev\n"],"metadata":{"id":"iRQc2YeUoAJN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Target words are typically chosen to represent two different groups that you want to compare for bias. WEAT calculates how strongly each set of target words is associated with each set of attribute words. If one set of target words is more closely associated with positive attribute words (e.g., 'joy', 'peace', 'love') than the other, this might indicate a bias in the embedding space.Quantitative Analysis: The strength of these associations is quantified using cosine similarity in the embedding space. This provides a numerical measure of bias, which is the WEAT score."],"metadata":{"id":"BSmteGLothRD"}},{"cell_type":"code","source":["!wget -c \"http://nlp.stanford.edu/data/glove.6B.zip\"\n","!unzip glove.6B.zip\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","glove_input_file = 'glove.6B.100d.txt'  # Adjust the file name as needed\n","word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n","glove2word2vec(glove_input_file, word2vec_output_file)"],"metadata":{"id":"wLWkEydkznBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_embeddings_model = KeyedVectors.load_word2vec_format('glove.6B.100d.txt.word2vec', binary=False)"],"metadata":{"id":"Kd1b-vwNX6UG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# male_words = ['he', 'him', 'his']   # Add more male-associated words\n","# female_words = ['she', 'her', 'hers']  # Add more female-associated words\n","# other_attribute_words = ['word1', 'word2', 'word3']  # Add neutral/contrasting words\n","\n","male_words = ['he', 'him', 'his', 'father', 'papa', 'dad', 'son', 'uncle', 'grandfather', 'grandpa', 'man', 'male', 'brother', 'husband', 'boyfriend', 'sir', 'king', 'guy', 'father-in-law', 'son-in-law', 'nephew', 'boy']\n","female_words = ['she', 'her', 'hers', 'mother', 'mama', 'daughter', 'aunt', 'auntie', 'grandmother', 'woman', 'female', 'sister', 'mom', 'wife', 'girlfriend', 'madam', 'queen', 'gal', 'niece', 'grandmother-in-law', 'daughter-in-law', 'lady', 'miss', 'sis', 'girl']\n","other_attribute_words = ['they', 'them', 'their', 'person', 'individual', 'someone', 'other', 'human', 'somebody', 'citizen']\n","\n","#This variable is supposed to represent another set of attribute words for the WEAT analysis, serving as a basis for comparison against the attribute words extracted from each topic.\n","male_words = [word for word in male_words if word in word_embeddings_model.key_to_index]\n","female_words = [word for word in female_words if word in word_embeddings_model.key_to_index]\n","other_attribute_words = [word for word in other_attribute_words if word in word_embeddings_model.key_to_index]"],"metadata":{"id":"cjUVU8a0tgW4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["if you want to display the topics as words instead of numeric labels in your WEAT analysis. you need to map the numeric topic labels to their corresponding words. You can achieve this by creating a mapping dictionary that associates each topic label with a list of words representing that topic."],"metadata":{"id":"lerhAJyISAfp"}},{"cell_type":"code","source":["def flatten(lst):\n","    return [item for sublist in lst for item in sublist]\n","\n","# Function to print the results in an organized manner\n","def print_weat_results(topic, male_association, female_association, weat_score, effect_size):\n","    print(f\"Topic: {topic}\")\n","    print(f\"  WEAT Score: {weat_score}\")\n","    print(f\"  Effect Size: {effect_size}\")\n","    print(f\"  Male Association: {male_association}\")\n","    print(f\"  Female Association: {female_association}\")\n","    print(\"\")\n","\n","# Create a mapping dictionary to associate topic labels with words\n","topic_words_mapping = {}  # Initialize an empty dictionary\n","\n","for topic_label in aggregated_lyrics['topic'].unique():\n","    # You can use your existing logic to obtain top words for each topic\n","    top_words = aggregated_lyrics[(aggregated_lyrics['topic'] == topic_label)]['top_words'].tolist()\n","    top_words = flatten(top_words)\n","    top_words_cleared = [item[0] for item in top_words]\n","    topic_words_mapping[topic_label] = top_words_cleared\n","\n","# Compute and print WEAT score and effect size for each topic\n","for topic in aggregated_lyrics['topic'].unique():\n","    attribute_set = []\n","    for gender in ['Male', 'Female']:\n","        top_words = aggregated_lyrics[(aggregated_lyrics['topic'] == topic) &\n","                                      (aggregated_lyrics['gender'] == gender)]['top_words'].tolist()\n","        top_words = flatten(top_words)\n","        top_words_cleared = [item[0] for item in top_words]\n","        attribute_set.extend(top_words_cleared)\n","\n","    if attribute_set:\n","        weat_score = differential_association(male_words, female_words, attribute_set, other_attribute_words, word_embeddings_model)\n","        effect_size = weat_effect_size(male_words, female_words, attribute_set, other_attribute_words, word_embeddings_model)\n","        #The function's purpose is to quantify potential biases in word embeddings. Specifically, it measures how much more\n","        # strongly one set of target words (e.g., male_words) is associated with a certain topic's words (attribute_set)\n","        # compared to another set of target words (e.g., female_words), and vice versa.\n","\n","        # Comparing Male and Female Words with Topic Words (attribute_set):\n","\n","        # The function calculates how strongly words related to males and females are associated with words from a specific topic in the lyrics.\n","        # This is done by computing the cosine similarity between the target words' embeddings and the average embedding of the topic's words.\n","        # Comparing Male and Female Words with Other Attribute Words (other_attribute_words):\n","\n","        # Similarly, the function assesses the association between the male and female words with another set of attribute words, which serves as a comparison or control group.\n","        # Effect Size Calculation:\n","\n","        # The effect size is computed to quantify the difference in association strengths. A larger effect size suggests a more pronounced bias,\n","        #  indicating that one set of target words (either male_words or female_words) has a stronger association with the topic words compared to the other set.\n","\n","        male_association = mean_embedding_similarity(male_words, attribute_set, word_embeddings_model)\n","        female_association = mean_embedding_similarity(female_words, attribute_set, word_embeddings_model)\n","        # Print the topic as words instead of the numeric label\n","        topic_words = ', '.join(topic_words_mapping.get(topic_label, []))\n","        print_weat_results(topic_words, male_association, female_association, weat_score, effect_size)\n","    else:\n","        print(f\"Topic {topic} - Not enough data for WEAT analysis\")"],"metadata":{"id":"e6qzol9usWsI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch\n","import torch"],"metadata":{"id":"TNzKTCR65v-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/gdrive/MyDrive/Bertopic/shared_work/model_checkpoint.pth')"],"metadata":{"id":"nEspcElw5eYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/gdrive/MyDrive/Bertopic/shared_work/model_checkpoint.pth'))"],"metadata":{"id":"FXoZ1LlT5fzL"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"bertopic","language":"python","name":"bertopic"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}